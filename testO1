import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import seaborn as sns
import time

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load MNIST Dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_set = torchvision.datasets.MNIST(root="./data", train=True, download=True, transform=transform)
test_set = torchvision.datasets.MNIST(root="./data", train=False, download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False)

# Define Standard CNN
class StandardCNN(nn.Module):
    def __init__(self):
        super(StandardCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(32 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2(x), 2))
        x = x.view(x.size(0), -1)  # Flatten
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

# Define Ψ-Sai Net (Placeholder for now, replace with Ψ-Sai Transformations)
class PsiSaiNet(nn.Module):
    def __init__(self):
        super(PsiSaiNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(32 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2(x), 2))
        x = x.view(x.size(0), -1)  # Flatten
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

# Training and Evaluation Function with Loss Tracking
def train_and_track_loss(model, train_loader, test_loader, epochs=5):
    model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    train_losses = []
    test_accuracies = []

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(train_loader)
        train_losses.append(avg_loss)

        # Evaluate Accuracy
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        accuracy = 100 * correct / total
        test_accuracies.append(accuracy)

        print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")

    return train_losses, test_accuracies

# Train Models and Store Results
start_time = time.time()
cnn_losses, cnn_accuracies = train_and_track_loss(StandardCNN(), train_loader, test_loader)
cnn_time = time.time() - start_time

start_time = time.time()
sai_losses, sai_accuracies = train_and_track_loss(PsiSaiNet(), train_loader, test_loader)
sai_time = time.time() - start_time

# Plot Loss and Accuracy Curves
epochs = range(1, 6)

plt.figure(figsize=(12, 5))

# Loss Curve
plt.subplot(1, 2, 1)
plt.plot(epochs, cnn_losses, 'r--', label="Standard CNN")
plt.plot(epochs, sai_losses, 'b-', label="Ψ-Sai Net")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Curve (Convergence Speed)")
plt.legend()

# Accuracy Curve
plt.subplot(1, 2, 2)
plt.plot(epochs, cnn_accuracies, 'r--', label="Standard CNN")
plt.plot(epochs, sai_accuracies, 'b-', label="Ψ-Sai Net")
plt.xlabel("Epochs")
plt.ylabel("Accuracy (%)")
plt.title("Test Accuracy Curve")
plt.legend()

plt.show()

# Gradient Flow Heatmap
def plot_gradient_flow(model, model_name):
    """ Visualizes the gradient flow through layers """
    model.zero_grad()
    images, labels = next(iter(train_loader))
    images, labels = images.to(device), labels.to(device)
    outputs = model(images)
    loss = nn.CrossEntropyLoss()(outputs, labels)
    loss.backward()

    grads = [p.grad.abs().mean().item() for p in model.parameters() if p.grad is not None]
    sns.heatmap([grads], cmap="coolwarm", xticklabels=False, yticklabels=[model_name])
    plt.title(f"Gradient Flow Heatmap ({model_name})")
    plt.show()

# Visualize Gradient Flow
plot_gradient_flow(PsiSaiNet(), "Ψ-Sai Net")
plot_gradient_flow(StandardCNN(), "Standard CNN")

# Feature Map Visualization
def visualize_feature_maps(model, model_name):
    """ Extracts and plots feature maps from the first conv layer """
    model.eval()
    images, _ = next(iter(test_loader))
    images = images.to(device)

    with torch.no_grad():
        feature_maps = model.conv1(images).cpu()

    fig, axes = plt.subplots(1, 6, figsize=(15, 5))
    for i in range(6):
        axes[i].imshow(feature_maps[0, i].detach().numpy(), cmap="viridis")
        axes[i].axis("off")
    plt.suptitle(f"Feature Maps ({model_name})")
    plt.show()

# Compare Feature Maps
visualize_feature_maps(StandardCNN(), "Standard CNN")
visualize_feature_maps(PsiSaiNet(), "Ψ-Sai Net")

# Final Results Summary
print("\n--- Benchmark Results ---")
print(f"Standard CNN Accuracy: {cnn_accuracies[-1]:.2f}%, Training Time: {cnn_time:.2f}s")
print(f"Ψ-Sai Net Accuracy: {sai_accuracies[-1]:.2f}%, Training Time: {sai_time:.2f}s")
