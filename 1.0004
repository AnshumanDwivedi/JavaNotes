import torch import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms from torch.utils.data import DataLoader

Load CIFAR-10 dataset

transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

trainloader = DataLoader(trainset, batch_size=64, shuffle=True) testloader = DataLoader(testset, batch_size=64, shuffle=False)

class PsiAdaptiveConv(nn.Module): def init(self, in_channels, out_channels, kernel_size): super(PsiAdaptiveConv, self).init() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1) self.psi_params = nn.Parameter(torch.ones(out_channels))  # Ψ-Adaptive parameters

def forward(self, x):
    weight = self.conv.weight * self.psi_params.view(1, -1, 1, 1)  # Ψ-Weight Adaptation
    return F.conv2d(x, weight, bias=self.conv.bias, padding=1)

class PsiAttention(nn.Module): def init(self, channels): super(PsiAttention, self).init() self.alpha = nn.Parameter(torch.ones(1)) self.beta = nn.Parameter(torch.ones(1))

def forward(self, conv_features, context_features):
    return self.alpha * conv_features + self.beta * context_features

class PsiStability(nn.Module): def init(self): super(PsiStability, self).init() self.gamma = nn.Parameter(torch.tensor(0.1))  # Stability control parameter

def forward(self, weights, gradients):
    return weights - self.gamma * gradients  # Stability-aware update

class PsiCNN(nn.Module): def init(self, num_classes=10): super(PsiCNN, self).init() self.conv1 = PsiAdaptiveConv(3, 32, 3) self.conv2 = PsiAdaptiveConv(32, 64, 3) self.attention = PsiAttention(64) self.stability = PsiStability() self.fc1 = nn.Linear(64 * 8 * 8, 256) self.fc2 = nn.Linear(256, num_classes)

def forward(self, x):
    x1 = F.relu(self.conv1(x))
    x2 = F.relu(self.conv2(x1))
    x3 = self.attention(x2, x1)  # Apply Ψ-Attention
    x3 = F.max_pool2d(x3, 2)
    x3 = torch.flatten(x3, 1)
    x3 = F.relu(self.fc1(x3))
    x3 = self.fc2(x3)
    return x3

def apply_stability_update(self):
    with torch.no_grad():
        for param in self.parameters():
            if param.grad is not None:
                param.copy_(self.stability(param, param.grad))  # Apply stability-aware weight update

Training setup

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") model = PsiCNN(num_classes=10).to(device) optimizer = torch.optim.Adam(model.parameters(), lr=0.001) criterion = nn.CrossEntropyLoss()

Training loop

num_epochs = 10 for epoch in range(num_epochs): model.train() running_loss = 0.0 for inputs, labels in trainloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() model.apply_stability_update()  # Apply Ψ-Stability update optimizer.step() running_loss += loss.item() print(f"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}")

Testing the model

model.eval() correct, total = 0, 0 with torch.no_grad(): for inputs, labels in testloader: inputs, labels = inputs.to(device), labels.to(device) outputs = model(inputs) _, predicted = torch.max(outputs, 1) total += labels.size(0) correct += (predicted == labels).sum().item()

print(f"Test Accuracy: {100 * correct / total:.2f}%")

